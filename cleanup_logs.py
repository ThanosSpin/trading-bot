#!/usr/bin/env python3
# cleanup_logs.py
"""
Cleanup old log files and prediction logs.
Run this daily via cron to keep disk usage under control.
"""

import os
import sys
from datetime import datetime, timedelta
from pathlib import Path

# Add bot directory to path
BOT_DIR = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, BOT_DIR)

from model_monitor import ModelMonitor


def cleanup_logs_directory(logs_dir: str = "logs", days_to_keep: int = 30):
    """
    Remove log files older than specified days from logs/ directory.

    Args:
        logs_dir: Path to logs directory
        days_to_keep: Keep logs from last N days
    """
    logs_path = os.path.join(BOT_DIR, logs_dir)

    if not os.path.exists(logs_path):
        print(f"[INFO] Logs directory not found: {logs_path}")
        return

    cutoff = datetime.now() - timedelta(days=days_to_keep)
    removed_count = 0
    removed_size = 0

    print(f"\nðŸ§¹ Cleaning up logs older than {days_to_keep} days...")
    print(f"   Cutoff date: {cutoff.strftime('%Y-%m-%d')}")
    print(f"   Directory: {logs_path}")
    print("-" * 60)

    for filename in os.listdir(logs_path):
        filepath = os.path.join(logs_path, filename)

        # Skip directories
        if os.path.isdir(filepath):
            continue

        try:
            # Get file modification time
            file_mtime = datetime.fromtimestamp(os.path.getmtime(filepath))

            if file_mtime < cutoff:
                file_size = os.path.getsize(filepath)
                os.remove(filepath)
                removed_count += 1
                removed_size += file_size
                print(f"  âœ… Removed: {filename} ({file_size/1024:.1f} KB, {file_mtime.strftime('%Y-%m-%d')})")

        except Exception as e:
            print(f"  âš ï¸ Error removing {filename}: {e}")

    if removed_count > 0:
        print("-" * 60)
        print(f"âœ… Removed {removed_count} files ({removed_size/1024/1024:.2f} MB)")
    else:
        print("âœ… No old files to remove")


def cleanup_prediction_logs(days_to_keep: int = 30):
    """
    Cleanup old prediction logs using ModelMonitor.

    Args:
        days_to_keep: Keep predictions from last N days
    """
    print(f"\nðŸ§¹ Cleaning up prediction logs older than {days_to_keep} days...")
    print("-" * 60)

    try:
        monitor = ModelMonitor()
        monitor.cleanup_old_logs(days_to_keep=days_to_keep)
        print("âœ… Prediction logs cleaned up")
    except Exception as e:
        print(f"âš ï¸ Could not cleanup prediction logs: {e}")


def cleanup_old_model_backups(model_dir: str = "models", max_backups: int = 6):
    """
    Keep only the most recent N backup files per symbol/mode.

    Backup files follow pattern: SYMBOL_MODE_xgb_YYYY-MM.pkl
    Active files: SYMBOL_MODE_xgb.pkl (no date)

    Args:
        model_dir: Path to models directory
        max_backups: Keep this many backups per symbol/mode
    """
    models_path = os.path.join(BOT_DIR, model_dir)

    if not os.path.exists(models_path):
        print(f"[INFO] Models directory not found: {models_path}")
        return

    print(f"\nðŸ§¹ Cleaning up old model backups (keep {max_backups} per symbol/mode)...")
    print("-" * 60)

    # Group backups by symbol/mode
    backups = {}

    for filename in os.listdir(models_path):
        # Match pattern: SYMBOL_MODE_xgb_YYYY-MM.pkl
        if filename.endswith('.pkl') and '_xgb_' in filename:
            parts = filename.replace('.pkl', '').split('_')

            # Extract symbol and mode (everything before last 2 parts which are date)
            # Example: NVDA_intraday_mom_xgb_2026-01.pkl
            # -> symbol=NVDA, mode=intraday_mom

            if len(parts) >= 4:
                # Last 2 parts are date (YYYY-MM), before that is 'xgb'
                date_part = parts[-1]  # e.g., "2026-01"

                # Skip if it's the active file (no date)
                if '-' not in date_part:
                    continue

                # Everything before '_xgb_DATE'
                key_parts = []
                for i, part in enumerate(parts):
                    if part == 'xgb':
                        break
                    key_parts.append(part)

                key = '_'.join(key_parts)

                if key not in backups:
                    backups[key] = []

                filepath = os.path.join(models_path, filename)
                backups[key].append((filename, filepath, os.path.getmtime(filepath)))

    # Cleanup old backups for each symbol/mode
    removed_count = 0

    for key, files in backups.items():
        # Sort by modification time (newest first)
        files.sort(key=lambda x: x[2], reverse=True)

        # Keep only max_backups
        if len(files) > max_backups:
            to_remove = files[max_backups:]

            for filename, filepath, _ in to_remove:
                try:
                    os.remove(filepath)
                    removed_count += 1
                    print(f"  âœ… Removed old backup: {filename}")
                except Exception as e:
                    print(f"  âš ï¸ Error removing {filename}: {e}")

    if removed_count > 0:
        print("-" * 60)
        print(f"âœ… Removed {removed_count} old model backups")
    else:
        print("âœ… No old backups to remove")


def main():
    """Run all cleanup tasks."""
    print("="*70)
    print(f"LOG CLEANUP - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("="*70)

    # 1. Cleanup logs/ directory (signals, etc.)
    cleanup_logs_directory(logs_dir="logs", days_to_keep=30)

    # 2. Cleanup prediction logs (models/*_predictions.csv)
    cleanup_prediction_logs(days_to_keep=30)

    # 3. Cleanup old model backups
    cleanup_old_model_backups(model_dir="models", max_backups=6)

    print("\n" + "="*70)
    print("âœ… CLEANUP COMPLETE")
    print("="*70)


if __name__ == "__main__":
    main()
